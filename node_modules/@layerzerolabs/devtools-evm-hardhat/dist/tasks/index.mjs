import { types as types$1, task, subtask } from 'hardhat/config';
import { TASK_COMPILE } from 'hardhat/builtin-tasks/task-names';
import { isLogLevel, setDefaultLogLevel, createLogger, promptToSelectMultiple, promptForText, pluralizeNoun, promptToContinue, printBoolean, printJson, isFile, createModuleLogger } from '@layerzerolabs/io-devtools';
import { printLogo, render, createProgressBar, printRecords } from '@layerzerolabs/io-devtools/swag';
import { formatEid, splitCommaSeparated, isDeepEqual, serializeDockerComposeSpec, createSignAndSend, formatOmniTransaction as formatOmniTransaction$1 } from '@layerzerolabs/devtools';
import pMemoize from 'p-memoize';
import { Artifacts } from 'hardhat/internal/artifacts';
import { Contract } from '@ethersproject/contracts';
import { makeZeroAddress, isEVMAddress, createAnvilCliOptions } from '@layerzerolabs/devtools-evm';
import { HardhatContext } from 'hardhat/internal/context';
import { Environment } from 'hardhat/internal/core/runtime-environment';
import '@nomiclabs/hardhat-ethers/internal/ethers-provider-wrapper';
import assert, { AssertionError } from 'assert';
import memoize from 'micro-memoize';
import { HardhatError } from 'hardhat/internal/core/errors';
import { ERRORS } from 'hardhat/internal/core/errors-list';
import { endpointIdToStage, Environment as Environment$1, Stage } from '@layerzerolabs/lz-definitions';
import 'hardhat-deploy/dist/src/type-extensions';
import 'hardhat/types/config';
import { join, resolve } from 'path';
import { pipe } from 'fp-ts/lib/function';
import * as R from 'fp-ts/Record';
import { spawnSync } from 'child_process';
import { mkdirSync, writeFileSync, rmSync } from 'fs';
import * as RR from 'fp-ts/ReadonlyRecord';
import { generate, createIncludeDirent, generatorTypeScript } from '@layerzerolabs/export-deployments';

// src/tasks/deploy.ts

// src/constants/tasks.ts
var TASK_LZ_DEPLOY = "lz:deploy";
var TASK_LZ_EXPORT_DEPLOYMENTS_TYPESCRIPT = "lz:export:deployments:typescript";
var SUBTASK_LZ_SIGN_AND_SEND = "::lz:sign-and-send";
var TASK_LZ_TEST_SIMULATION_START = "lz:test:simulation:start";
var TASK_LZ_TEST_SIMULATION_LOGS = "lz:test:simulation:logs";
var TASK_LZ_TEST_SIMULATION_STOP = "lz:test:simulation:stop";

// src/errors/errors.ts
var ConfigurationError = class extends Error {
};
var getAllArtifacts = pMemoize(async (hre = getDefaultRuntimeEnvironment()) => {
  var _a, _b;
  const externalContracts = (_b = (_a = hre.config.external) == null ? void 0 : _a.contracts) != null ? _b : [];
  const artifactsPaths = [
    hre.config.paths.artifacts,
    hre.config.paths.imports,
    ...externalContracts.flatMap(({ artifacts }) => artifacts)
  ];
  const artifactsObjects = artifactsPaths.map((path) => new Artifacts(path));
  const artifactses = await Promise.all(artifactsObjects.map(getAllArtifactsFrom));
  return artifactses.flat();
});
var getAllArtifactsFrom = async (artifactsObject) => {
  const fullyQualifiedNames = await artifactsObject.getAllFullyQualifiedNames();
  return fullyQualifiedNames.map((name) => artifactsObject.readArtifactSync(name));
};
var isErrorFragment = (fragment) => fragment.type === "error";
pMemoize(async () => {
  const artifacts = await getAllArtifacts();
  const abi = artifacts.flatMap((artifact) => artifact.abi).filter(isErrorFragment);
  const deduplicatedAbi = Object.values(Object.fromEntries(abi.map((abi2) => [JSON.stringify(abi2), abi2])));
  return { eid: -1, contract: new Contract(makeZeroAddress(), deduplicatedAbi) };
});
var getDefaultContext = () => {
  try {
    return HardhatContext.getHardhatContext();
  } catch (error) {
    throw new ConfigurationError(`Could not get Hardhat context: ${error}`);
  }
};
var getDefaultRuntimeEnvironment = () => {
  const context = getDefaultContext();
  try {
    return context.getHardhatRuntimeEnvironment();
  } catch (error) {
    throw new ConfigurationError(`Could not get Hardhat Runtime Environment: ${error}`);
  }
};
var getHreByNetworkName = pMemoize(async (networkName) => {
  const context = getDefaultContext();
  const environment2 = getDefaultRuntimeEnvironment();
  try {
    return new Environment(
      environment2.config,
      {
        ...environment2.hardhatArguments,
        network: networkName
      },
      environment2.tasks,
      environment2.scopes,
      context.environmentExtenders,
      context.experimentalHardhatNetworkMessageTraceHooks,
      environment2.userConfig,
      context.providerExtenders
      // This is a bit annoying - the environmentExtenders are not stronly typed
      // so TypeScript complains that the properties required by HardhatRuntimeEnvironment
      // are not present on HardhatRuntimeEnvironmentImplementation
    );
  } catch (error) {
    throw new ConfigurationError(`Could not setup Hardhat Runtime Environment: ${error}`);
  }
});
var getNetworkNameForEid = (eid, hre = getDefaultRuntimeEnvironment()) => {
  const eidsByNetworkName = getEidsByNetworkName(hre);
  for (const [networkName, networkEid] of Object.entries(eidsByNetworkName)) {
    if (networkEid === eid) {
      return networkName;
    }
  }
  assert(false, `Could not find a network for eid ${eid} (${formatEid(eid)})`);
};
var getEidsByNetworkName = memoize(
  (hre = getDefaultRuntimeEnvironment()) => {
    const networkEntries = Object.entries(hre.config.networks);
    const eidEntries = networkEntries.map(
      ([networkName, networkConfig]) => [networkName, networkConfig.eid]
    );
    const eidsByNetworkName = Object.fromEntries(eidEntries);
    const eidEntriesWithDefinedEid = eidEntries.filter(([_, eid]) => eid != null);
    const definedEidsByNetworkName = Object.fromEntries(eidEntriesWithDefinedEid);
    const allDefinedEids = new Set(Object.values(definedEidsByNetworkName));
    const allNetworkNames = new Set(Object.keys(definedEidsByNetworkName));
    if (allDefinedEids.size === allNetworkNames.size) {
      return eidsByNetworkName;
    }
    const duplicatedNetworkNames = Array.from(allDefinedEids).map(
      (eid) => eidEntriesWithDefinedEid.flatMap(
        ([networkName, definedEid]) => eid === definedEid ? [networkName] : []
      )
    ).filter((networkNames) => networkNames.length > 1);
    const messages = duplicatedNetworkNames.map(
      (networkNames) => `- ${networkNames.join(", ")} have eid set to ${formatEid(eidsByNetworkName[networkNames[0]])}`
    ).join("\n");
    throw new Error(
      `Found multiple networks configured with the same 'eid':

${messages}

Please fix this in your hardhat config.`
    );
  }
);
var csv = {
  name: "csv",
  parse(name, value) {
    return splitCommaSeparated(value);
  },
  validate() {
  }
};
var isEnvironment = (value) => Object.values(Environment$1).includes(value);
var environment = {
  name: "environment",
  parse(name, value) {
    if (!isEnvironment(value)) {
      throw new HardhatError(ERRORS.ARGUMENTS.INVALID_VALUE_FOR_TYPE, {
        value,
        name,
        type: "environment"
      });
    }
    return value;
  },
  validate() {
  }
};
var isStage = (value) => Object.values(Stage).includes(value);
var stage = {
  name: "stage",
  parse(name, value) {
    if (!isStage(value)) {
      throw new HardhatError(ERRORS.ARGUMENTS.INVALID_VALUE_FOR_TYPE, {
        value,
        name,
        type: "stage"
      });
    }
    return value;
  },
  validate() {
  }
};
var logLevel = {
  name: "logLevel",
  parse(name, value) {
    if (!isLogLevel(value)) {
      throw new HardhatError(ERRORS.ARGUMENTS.INVALID_VALUE_FOR_TYPE, {
        value,
        name,
        type: "logLevel"
      });
    }
    return value;
  },
  validate() {
  }
};
var fn = {
  name: "function",
  parse: (argName, value) => {
    if (typeof value !== "function") {
      throw new HardhatError(ERRORS.ARGUMENTS.INVALID_VALUE_FOR_TYPE, {
        value,
        name: argName,
        type: fn.name
      });
    }
    return value;
  },
  validate() {
  }
};
var signer = {
  name: "signer",
  parse: (argName, value) => {
    if (isEVMAddress(value)) {
      return { type: "address", address: value };
    }
    const parsed = parseInt(value, 10);
    if (!isNaN(parsed)) {
      if (parsed < 0) {
        throw new HardhatError(ERRORS.ARGUMENTS.INVALID_VALUE_FOR_TYPE, {
          value,
          name: argName,
          type: signer.name
        });
      }
      return { type: "index", index: parsed };
    }
    return { type: "named", name: value };
  },
  validate() {
  }
};
var types = { csv, logLevel, fn, signer, environment, stage, ...types$1 };
function assertHardhatDeploy(hre) {
  assert(hre.deployments, `You don't seem to be using hardhat-deploy in your project`);
}
function assertDefinedNetworks(networkNames, hre = getDefaultRuntimeEnvironment()) {
  const definedNetworkNames = new Set(Object.keys(getEidsByNetworkName(hre)));
  for (const networkName of networkNames) {
    if (definedNetworkNames.has(networkName)) {
      continue;
    }
    throw new AssertionError({
      message: `Network '${networkName}' has not been defined. Defined networks are ${Array.from(definedNetworkNames).join(", ")}`
    });
  }
  return networkNames;
}
var action = async ({ networks: networksArgument, tags: tagsArgument = [], logLevel: logLevel2 = "info", ci = false, reset = false, stage: stage2 }, hre) => {
  printLogo();
  assertDefinedNetworks(networksArgument != null ? networksArgument : []);
  setDefaultLogLevel(logLevel2);
  const logger = createLogger();
  const isInteractive = !ci;
  logger.debug(isInteractive ? "Running in interactive mode" : "Running in non-interactive (CI) mode");
  logger.debug(reset ? "Will delete existing deployments" : "Will not delete existing deployments");
  try {
    logger.info(`Compiling you hardhat project`);
    await hre.run(TASK_COMPILE);
  } catch (error) {
    logger.warn(`Failed to compile the project: ${error}`);
  }
  if (networksArgument != null && stage2 != null) {
    logger.error(`--stage ${stage2} cannot be used in conjunction with --networks ${networksArgument.join(",")}`);
    process.exit(1);
  }
  const eidsByNetworks = Object.entries(getEidsByNetworkName());
  const filteredEidsByNetworks = stage2 == null ? eidsByNetworks : eidsByNetworks.filter(([, eid]) => eid != null && endpointIdToStage(eid) === stage2);
  const configuredNetworkNames = filteredEidsByNetworks.flatMap(([name, eid]) => eid == null ? [] : [name]);
  const networks = networksArgument != null ? networksArgument : configuredNetworkNames;
  let selectedNetworks;
  let selectedTags;
  if (isInteractive) {
    const networksSet = new Set(networks);
    const options = eidsByNetworks.map(([networkName, eid]) => ({
      title: networkName,
      value: networkName,
      disabled: eid == null,
      selected: networksSet.has(networkName),
      hint: eid == null ? void 0 : `Connected to ${formatEid(eid)}`
    })).sort(
      (a, b) => (
        // We want to show the enabled networks first
        Number(a.disabled) - Number(b.disabled) || //  And sort the rest by their name
        a.title.localeCompare(b.title)
      )
    );
    selectedNetworks = await promptToSelectMultiple("Which networks would you like to deploy?", { options });
    selectedTags = await promptForText("Which deploy script tags would you like to use?", {
      defaultValue: tagsArgument == null ? void 0 : tagsArgument.join(","),
      hint: "Leave empty to use all deploy scripts"
    }).then(splitCommaSeparated);
  } else {
    selectedNetworks = networks;
    selectedTags = tagsArgument;
  }
  if (selectedNetworks.length === 0) {
    return logger.warn(`No networks selected, exiting`), {};
  }
  logger.info(
    pluralizeNoun(
      selectedNetworks.length,
      `Will deploy 1 network: ${selectedNetworks.join(",")}`,
      `Will deploy ${selectedNetworks.length} networks: ${selectedNetworks.join(", ")}`
    )
  );
  if (selectedTags.length === 0) {
    logger.warn(`Will use all deployment scripts`);
  } else {
    logger.info(`Will use deploy scripts tagged with ${selectedTags.join(", ")}`);
  }
  const shouldDeploy = isInteractive ? await promptToContinue() : true;
  if (!shouldDeploy) {
    return logger.verbose(`User cancelled the operation, exiting`), {};
  }
  logger.verbose(`Running deployment scripts`);
  const progressBar = render(createProgressBar({ before: "Deploying... ", after: ` 0/${selectedNetworks.length}` }));
  let numProcessed = 0;
  const results = {};
  await Promise.all(
    selectedNetworks.map(async (networkName) => {
      const env = await getHreByNetworkName(networkName);
      try {
        assertHardhatDeploy(env);
        const deploymentsBefore = await env.deployments.all();
        const deploymentsAfter = await env.deployments.run(selectedTags, {
          // If we don't pass resetmemory or set it to true,
          // hardhat deploy will erase the database of deployments
          // (including the external deployments)
          //
          // In effect this means the deployments for LayerZero artifacts would not be available
          resetMemory: false,
          writeDeploymentsToFiles: true,
          deletePreviousDeployments: reset
        });
        const contracts = Object.fromEntries(
          Object.entries(deploymentsAfter).filter(
            ([name]) => !isDeepEqual(deploymentsBefore[name], deploymentsAfter[name])
          )
        );
        results[networkName] = { contracts };
        logger.debug(`Successfully deployed network ${networkName}`);
      } catch (error) {
        results[networkName] = { error };
        logger.debug(`Failed deploying network ${networkName}: ${error}`);
      } finally {
        numProcessed++;
        progressBar.rerender(
          createProgressBar({
            before: "Deploying... ",
            after: ` ${numProcessed}/${selectedNetworks.length}`,
            progress: numProcessed
          })
        );
      }
    })
  );
  progressBar.clear();
  const errors = Object.entries(results).flatMap(
    ([networkName, { error }]) => error == null ? [] : [{ networkName, error }]
  );
  if (errors.length === 0) {
    return logger.info(`${printBoolean(true)} Your contracts are now deployed`), results;
  }
  logger.error(
    `${printBoolean(false)} ${pluralizeNoun(errors.length, "Failed to deploy 1 network", `Failed to deploy ${errors.length} networks`)}`
  );
  const previewErrors = isInteractive ? await promptToContinue(`Would you like to see the deployment errors?`) : true;
  if (previewErrors) {
    printRecords(
      errors.map(({ networkName, error }) => ({
        Network: networkName,
        Error: String(error)
      }))
    );
  }
  process.exitCode = process.exitCode || 1;
  return results;
};
task(TASK_LZ_DEPLOY, "Deploy LayerZero contracts", action).addParam(
  "networks",
  "List of comma-separated networks. If not provided, all networks will be deployed",
  void 0,
  types.csv,
  true
).addParam(
  "tags",
  "List of comma-separated deploy script tags to deploy. If not provided, all deploy scripts will be executed",
  void 0,
  types.csv,
  true
).addParam("logLevel", "Logging level. One of: error, warn, info, verbose, debug, silly", "info", types.logLevel).addParam("stage", "Chain stage. One of: mainnet, testnet, sandbox", void 0, types.stage, true).addFlag("ci", "Continuous integration (non-interactive) mode. Will not ask for any input from the user").addFlag("reset", "Delete existing deployments");
var resolveSimulationConfig = (userConfig, hardhatConfig) => {
  var _a, _b;
  return {
    port: (_a = userConfig.port) != null ? _a : 8545,
    directory: resolve(hardhatConfig.paths.root, (_b = userConfig.directory) != null ? _b : ".layerzero"),
    anvil: {
      // For now we'll hardcode the mnemonic we'll use to seed the accounts on the simulation networks
      mnemonic: "test test test test test test test test test test test junk",
      ...userConfig.anvil,
      // The host and port need to always point to 0.0.0.0:8545
      // since anvil runs in the container that exposes this port on 0.0.0.0
      host: "0.0.0.0",
      port: 8545
    }
  };
};
var getAnvilOptionsFromHardhatNetworks = (config, networksConfig) => pipe(
  networksConfig,
  // We want to drop all the networks that don't have URLs
  R.filter(isHttpNetworkConfig),
  // And map the network configs into AnvilOptions
  R.map(
    (networkConfig) => ({
      ...config.anvil,
      forkUrl: networkConfig.url
    })
  )
);
var pickNetworkConfigs = (networks) => R.filterWithIndex((networkName) => networks.includes(networkName));
var isHttpNetworkConfig = (networkConfig) => "url" in networkConfig && typeof networkConfig.url === "string";
var action2 = async ({ logLevel: logLevel2 = "info" }, hre) => {
  var _a, _b, _c;
  setDefaultLogLevel(logLevel2);
  printLogo();
  const logger = createLogger();
  const simulationUserConfig = (_c = (_b = (_a = hre.userConfig.layerZero) == null ? void 0 : _a.experimental) == null ? void 0 : _b.simulation) != null ? _c : {};
  logger.verbose(`Using simulation user config:
${printJson(simulationUserConfig)}`);
  const simulationConfig = resolveSimulationConfig(simulationUserConfig, hre.config);
  logger.verbose(`Resolved simulation config:
${printJson(simulationConfig)}`);
  const dockerComposePath = join(simulationConfig.directory, "docker-compose.yaml");
  if (!isFile(dockerComposePath)) {
    logger.warn(`Could not find simulation docker compose file '${dockerComposePath}'`);
    logger.warn(`Did you run 'npx hardhat ${TASK_LZ_TEST_SIMULATION_START}'?`);
    process.exitCode = 1;
    return;
  }
  try {
    logger.verbose(`Spawning docker compose logs command for ${dockerComposePath}`);
    spawnSync("docker", ["compose", "-f", dockerComposePath, "logs"], {
      stdio: "inherit"
    });
  } catch (error) {
    logger.error(`Failed to spawn docker compose logs command for ${dockerComposePath}: ${error}`);
    process.exitCode = 1;
    return;
  }
};
if (process.env.LZ_ENABLE_EXPERIMENTAL_SIMULATION) {
  task(TASK_LZ_TEST_SIMULATION_LOGS, "Show logs for LayerZero omnichain simulation", action2).addParam(
    "logLevel",
    "Logging level. One of: error, warn, info, verbose, debug, silly",
    "info",
    types.logLevel
  );
}
var createEvmNodeServiceSpec = (anvilOptions) => ({
  // This service references a Dockerfile that is copied
  // next to the resulting docker-compose.yaml
  //
  // The source for this Dockerfile is located in src/simulation/assets/Dockerfile.conf
  build: {
    dockerfile: "Dockerfile",
    target: "node-evm"
  },
  command: ["anvil", ...createAnvilCliOptions(anvilOptions)]
});
var createEvmNodeProxyServiceSpec = (port, networkServices) => ({
  // This service references a Dockerfile that is copied
  // next to the resulting docker-compose.yaml
  //
  // The source for this Dockerfile is located in src/simulation/assets/Dockerfile.conf
  build: {
    dockerfile: "Dockerfile",
    target: "proxy-evm"
  },
  // This service will expose its internal 8545 port to a host port
  //
  // The internal 8545 port is hardcoded both here and in the nginx.conf file,
  // the source for which is located in src/simulation/assets/nginx.conf
  ports: [`${port}:8545`],
  depends_on: pipe(
    networkServices,
    // This service will depend on the RPCs to be healthy
    // so we'll take the networkServices object and replace
    // the values with service_healthy condition
    RR.map(() => ({
      condition: "service_healthy"
    }))
  )
});
var createSimulationComposeSpec = (config, networks) => ({
  version: "3.9",
  services: pipe(
    networks,
    // First we turn the networks into docker compose specs for EVM nodes
    RR.map(createEvmNodeServiceSpec),
    (networkServiceSpecs) => (
      // Then we add the RPC proxy server
      //
      // There is a small edge case here that we can address
      // if it ever comes up: if a network is called 'rpc', this compose file
      // will not work.
      //
      // The fix for this is to prefix all networks with something like network-xxx
      // but we can do that if ever this usecase comes up
      pipe(
        networkServiceSpecs,
        RR.upsertAt("rpc", createEvmNodeProxyServiceSpec(config.port, networkServiceSpecs))
      )
    )
  )
});

// src/simulation/assets/Dockerfile.conf
var Dockerfile_default = "ARG FOUNDRY_VERSION=nightly-156cb1396b7076c6f9cb56f3719f8c90f7f52064\nARG ALPINE_VERSION=3.18\n\n#   .-.-.   .-.-.   .-.-.   .-.-.   .-.-.   .-.-.   .-.-.   .-.-\n#  / / \\ \\ / / \\ \\ / / \\ \\ / / \\ \\ / / \\ \\ / / \\ \\ / / \\ \\ / / \\\n# `-'   `-`-'   `-`-'   `-`-'   `-`-'   `-`-'   `-`-'   `-`-'\n#\n#             Image that gives us the foundry tools\n#\n#   .-.-.   .-.-.   .-.-.   .-.-.   .-.-.   .-.-.   .-.-.   .-.-\n#  / / \\ \\ / / \\ \\ / / \\ \\ / / \\ \\ / / \\ \\ / / \\ \\ / / \\ \\ / / \\\n# `-'   `-`-'   `-`-'   `-`-'   `-`-'   `-`-'   `-`-'   `-`-'\nFROM ghcr.io/foundry-rs/foundry:$FOUNDRY_VERSION AS foundry\n\n#   .-.-.   .-.-.   .-.-.   .-.-.   .-.-.   .-.-.   .-.-.   .-.-\n#  / / \\ \\ / / \\ \\ / / \\ \\ / / \\ \\ / / \\ \\ / / \\ \\ / / \\ \\ / / \\\n# `-'   `-`-'   `-`-'   `-`-'   `-`-'   `-`-'   `-`-'   `-`-'\n#\n#               Image that starts an EVM node\n#\n#   .-.-.   .-.-.   .-.-.   .-.-.   .-.-.   .-.-.   .-.-.   .-.-\n#  / / \\ \\ / / \\ \\ / / \\ \\ / / \\ \\ / / \\ \\ / / \\ \\ / / \\ \\ / / \\\n# `-'   `-`-'   `-`-'   `-`-'   `-`-'   `-`-'   `-`-'   `-`-'\nFROM alpine:$ALPINE_VERSION AS node-evm\n\nSTOPSIGNAL SIGINT\n\n# We will provide a default healthcheck (that assumes that the netowrk is running on the default port 8545)\nHEALTHCHECK --timeout=2s --interval=2s --retries=20 CMD cast block --rpc-url http://localhost:8545/ latest\n\n# Get anvil\nCOPY --from=foundry /usr/local/bin/anvil /usr/local/bin/anvil\n\n# Get cast for healthcheck\nCOPY --from=foundry /usr/local/bin/cast /usr/local/bin/cast\n\n#   .-.-.   .-.-.   .-.-.   .-.-.   .-.-.   .-.-.   .-.-.   .-.-\n#  / / \\ \\ / / \\ \\ / / \\ \\ / / \\ \\ / / \\ \\ / / \\ \\ / / \\ \\ / / \\\n# `-'   `-`-'   `-`-'   `-`-'   `-`-'   `-`-'   `-`-'   `-`-'\n#\n#           Image that starts an nginx proxy server\n#\n#   .-.-.   .-.-.   .-.-.   .-.-.   .-.-.   .-.-.   .-.-.   .-.-\n#  / / \\ \\ / / \\ \\ / / \\ \\ / / \\ \\ / / \\ \\ / / \\ \\ / / \\ \\ / / \\\n# `-'   `-`-'   `-`-'   `-`-'   `-`-'   `-`-'   `-`-'   `-`-'\nFROM nginx:alpine$ALPINE_VERSION AS proxy-evm\n\nCOPY ./nginx.conf /etc/nginx/nginx.conf\n\nHEALTHCHECK --timeout=2s --interval=2s --retries=20 CMD curl -f http://0.0.0.0:8545/health-check\n\n";

// src/simulation/assets/nginx.conf
var nginx_default = `events {}

http {
  # We will modify the log  format to include the target_network
  log_format proxied '$remote_addr - $remote_user [$time_local] '
                     '"$request" $status $body_bytes_sent '
                     '"$http_referer" "$http_user_agent" '
                     'Network: "$target_network"';

  server {
    # This proxy server will listen on port 8545
    # 
    # Even though it's not ideal to have this hardcoded, this port
    # will be remapped to a desired host port using docker compose,
    # the only issue this hardcoding brings is the fact that this port
    # needs to match the container port in the compose spec
    listen 8545;
    listen [::]:8545;

    # We will add a simple endpoint for healthcheck
    location /health-check {
      access_log	off;
      error_log	off;
      return 200 'ok';
    }

    # In this section we'll proxy all the requests to this server
    # to the respective network nodes
    # 
    # The requests are proxied based on the first path segment:
    # 
    # http://localhost/fuji -> http://fuji:8545/
    # 
    # For now the remaining path segments are not being preserved:
    # 
    # # http://localhost/fuji/some/url -> http://fuji:8545/
    location / {
      # Set the log format to be our custom 'proxied' log format
      access_log /var/log/nginx/access.log proxied;

      resolver 127.0.0.11;
      autoindex off;

      # This variable will hold the name of the network to proxy to
      set $target_network '';

      # Extract the first path segment from the request URI
      if ($request_uri ~* ^/(?<target_network>[^/]+)(/.*)?$) {
        set $target_network $1;
      }

      # Proxy the request to the appropriate network
      proxy_pass http://$target_network:8545/;
    }
  }
}`;
var action3 = async ({ networks: networksArgument, daemon = false, logLevel: logLevel2 = "info" }, hre) => {
  var _a, _b, _c;
  setDefaultLogLevel(logLevel2);
  printLogo();
  const logger = createLogger();
  const networks = networksArgument ? (
    // Here we need to check whether the networks have been defined in hardhat config
    assertDefinedNetworks(networksArgument)
  ) : (
    //  But here we are taking them from hardhat config so no assertion is necessary
    Object.entries(getEidsByNetworkName(hre)).flatMap(([networkName, eid]) => eid == null ? [] : [networkName])
  );
  if (networks.length === 0) {
    logger.warn(`No networks with eid configured, exiting`);
    return;
  }
  logger.info(`Will create a simulation configuration for networks ${networks.join(", ")}`);
  const simulationUserConfig = (_c = (_b = (_a = hre.userConfig.layerZero) == null ? void 0 : _a.experimental) == null ? void 0 : _b.simulation) != null ? _c : {};
  logger.verbose(`Using simulation user config:
${printJson(simulationUserConfig)}`);
  const simulationConfig = resolveSimulationConfig(simulationUserConfig, hre.config);
  logger.verbose(`Resolved simulation config:
${printJson(simulationConfig)}`);
  const networkConfigs = pickNetworkConfigs(networks)(hre.config.networks);
  const anvilOptions = getAnvilOptionsFromHardhatNetworks(simulationConfig, networkConfigs);
  logger.verbose(`The anvil config is:
${printJson(anvilOptions)}`);
  const composeSpec = createSimulationComposeSpec(simulationConfig, anvilOptions);
  const serializedComposeSpec = serializeDockerComposeSpec(composeSpec);
  logger.verbose(`Making sure directory ${simulationConfig.directory} exists`);
  mkdirSync(simulationConfig.directory, { recursive: true });
  const dockerfilePath = join(simulationConfig.directory, "Dockerfile");
  logger.debug(`Writing simulation Dockerfile to ${dockerfilePath}`);
  writeFileSync(dockerfilePath, Dockerfile_default);
  const nginxConfPath = join(simulationConfig.directory, "nginx.conf");
  logger.debug(`Writing simulation nginx configuration file to ${nginxConfPath}`);
  writeFileSync(nginxConfPath, nginx_default);
  const dockerComposePath = join(simulationConfig.directory, "docker-compose.yaml");
  logger.debug(`Writing simulation docker compose spec file to ${dockerComposePath}`);
  writeFileSync(dockerComposePath, serializedComposeSpec);
  try {
    if (daemon) {
      logger.info(`Starting simulation in the background`);
      logger.info(
        `Use 'LZ_ENABLE_EXPERIMENTAL_SIMULATION=1 npx hardhat ${TASK_LZ_TEST_SIMULATION_LOGS}' to view the network logs`
      );
    } else {
      logger.info(`Starting simulation`);
    }
    logger.verbose(`Spawning docker compose up command for ${dockerComposePath}`);
    const additionalUpArgs = daemon ? ["--wait"] : [];
    spawnSync("docker", ["compose", "-f", dockerComposePath, "up", ...additionalUpArgs], {
      stdio: "inherit"
    });
  } catch (error) {
    logger.error(`Failed to spawn docker compose up command for ${dockerComposePath}: ${error}`);
    process.exitCode = 1;
    return;
  }
};
if (process.env.LZ_ENABLE_EXPERIMENTAL_SIMULATION) {
  task(TASK_LZ_TEST_SIMULATION_START, "Start LayzerZero omnichain simulation", action3).addParam("logLevel", "Logging level. One of: error, warn, info, verbose, debug, silly", "info", types.logLevel).addParam("networks", "Comma-separated list of networks to simulate", void 0, types.csv, true).addFlag("daemon", "Start the simulation in the background");
}
var action4 = async ({ logLevel: logLevel2 = "info" }, hre) => {
  var _a, _b, _c;
  setDefaultLogLevel(logLevel2);
  printLogo();
  const logger = createLogger();
  const simulationUserConfig = (_c = (_b = (_a = hre.userConfig.layerZero) == null ? void 0 : _a.experimental) == null ? void 0 : _b.simulation) != null ? _c : {};
  logger.verbose(`Using simulation user config:
${printJson(simulationUserConfig)}`);
  const simulationConfig = resolveSimulationConfig(simulationUserConfig, hre.config);
  logger.verbose(`Resolved simulation config:
${printJson(simulationConfig)}`);
  const dockerComposePath = join(simulationConfig.directory, "docker-compose.yaml");
  if (!isFile(dockerComposePath)) {
    logger.warn(`Could not find simulation docker compose file '${dockerComposePath}'`);
    logger.warn(`Did you run 'npx hardhat ${TASK_LZ_TEST_SIMULATION_START}'?`);
    process.exitCode = 1;
    return;
  }
  try {
    logger.info(`Stopping simulation`);
    logger.verbose(`Spawning docker compose down command for ${dockerComposePath}`);
    spawnSync("docker", ["compose", "-f", dockerComposePath, "down"], {
      stdio: "inherit"
    });
  } catch (error) {
    logger.error(`Failed to spawn docker compose down command for ${dockerComposePath}: ${error}`);
    process.exitCode = 1;
    return;
  } finally {
    try {
      rmSync(simulationConfig.directory, { force: true, recursive: true });
    } catch (error) {
      logger.error(`Failed to delete '${simulationConfig.directory}': ${error}`);
    }
  }
};
if (process.env.LZ_ENABLE_EXPERIMENTAL_SIMULATION) {
  task(TASK_LZ_TEST_SIMULATION_STOP, "Stop LayerZero omnichain simulation", action4).addParam(
    "logLevel",
    "Logging level. One of: error, warn, info, verbose, debug, silly",
    "info",
    types.logLevel
  );
}
var formatOmniTransaction = (transaction) => ({
  Network: getNetworkNameForEid(transaction.point.eid),
  ...formatOmniTransaction$1(transaction)
});
var action5 = async ({
  ci,
  transactions,
  createSigner
}) => {
  const isInteractive = !ci;
  const logger = createLogger();
  const subtaskLogger = createModuleLogger(SUBTASK_LZ_SIGN_AND_SEND);
  const previewTransactions = isInteractive ? await promptToContinue(`Would you like to preview the transactions before continuing?`) : true;
  if (previewTransactions) {
    printRecords(transactions.map(formatOmniTransaction));
  }
  const shouldSubmit = isInteractive ? await promptToContinue(`Would you like to submit the required transactions?`) : true;
  if (!shouldSubmit) {
    return subtaskLogger.verbose(`User cancelled the operation, exiting`), [[], [], transactions];
  }
  subtaskLogger.verbose(`Signing and sending transactions:

${printJson(transactions)}`);
  const signAndSend = createSignAndSend(createSigner);
  let transactionsToSign = transactions;
  let successfulTransactions = [];
  let errors = [];
  while (true) {
    const progressBar = render(
      createProgressBar({ before: "Signing... ", after: ` 0/${transactionsToSign.length}` })
    );
    subtaskLogger.verbose(`Sending the transactions`);
    const [successfulBatch, errorsBatch, pendingBatch] = await signAndSend(
      transactionsToSign,
      (result, results) => {
        progressBar.rerender(
          createProgressBar({
            progress: results.length / transactionsToSign.length,
            before: "Signing... ",
            after: ` ${results.length}/${transactionsToSign.length}`
          })
        );
      }
    );
    progressBar.clear();
    successfulTransactions = [...successfulTransactions, ...successfulBatch];
    errors = errorsBatch;
    transactionsToSign = pendingBatch;
    subtaskLogger.verbose(`Sent the transactions`);
    subtaskLogger.debug(`Successfully sent the following transactions:

${printJson(successfulBatch)}`);
    subtaskLogger.debug(`Failed to send the following transactions:

${printJson(errorsBatch)}`);
    subtaskLogger.debug(`Did not send the following transactions:

${printJson(pendingBatch)}`);
    logger.info(
      pluralizeNoun(
        successfulBatch.length,
        `Successfully sent 1 transaction`,
        `Successfully sent ${successfulBatch.length} transactions`
      )
    );
    if (errors.length === 0) {
      logger.info(`${printBoolean(true)} Your OApp is now configured`);
      break;
    }
    logger.error(
      pluralizeNoun(errors.length, `Failed to send 1 transaction`, `Failed to send ${errors.length} transactions`)
    );
    const previewErrors = isInteractive ? await promptToContinue(`Would you like to preview the failed transactions?`) : true;
    if (previewErrors) {
      printRecords(
        errors.map(({ error, transaction }) => ({
          error: String(error),
          ...formatOmniTransaction(transaction)
        }))
      );
    }
    const retry = isInteractive ? await promptToContinue(`Would you like to retry?`, true) : false;
    if (!retry) {
      logger.error(`${printBoolean(false)} Failed to configure the OApp`);
      break;
    }
  }
  return [successfulTransactions, errors, transactionsToSign];
};
subtask(SUBTASK_LZ_SIGN_AND_SEND, "Sign and send a list of transactions using a local signer", action5).addFlag("ci", "Continuous integration (non-interactive) mode. Will not ask for any input from the user").addParam("transactions", "List of OmniTransaction objects", void 0, types.any).addParam("createSigner", "Function that creates a signer for a particular network", void 0, types.fn);
var action6 = async ({ networks, contracts, logLevel: logLevel2 = "info", outDir = "generated" }, hre) => {
  printLogo();
  setDefaultLogLevel(logLevel2);
  const logger = createLogger();
  const results = generate({
    // Since we are in a hardhat project, the deployments path is coming from the config
    deploymentsDir: hre.config.paths.deployments,
    outDir,
    includeDeploymentFile: createIncludeDirent(contracts),
    includeNetworkDir: createIncludeDirent(networks),
    generator: generatorTypeScript
  });
  logger.info(
    `${printBoolean(true)} ${pluralizeNoun(results.length, `Generated 1 file:`, `Generated ${results.length} files`)}`
  );
  for (const { path } of results) {
    logger.info(`	${path}`);
  }
  return results;
};
task(TASK_LZ_EXPORT_DEPLOYMENTS_TYPESCRIPT, "Export deployments as TypeScript files", action6).addParam(
  "networks",
  "List of comma-separated networks. If not provided, all networks will be deployed",
  void 0,
  types.csv,
  true
).addParam(
  "contracts",
  "List of comma-separated contract names. If not provided, all contracts will be exported",
  void 0,
  types.csv,
  true
).addParam("logLevel", "Logging level. One of: error, warn, info, verbose, debug, silly", "info", types.logLevel);
//# sourceMappingURL=out.js.map
//# sourceMappingURL=index.mjs.map